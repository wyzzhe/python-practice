#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·˜å®å•†å“å“ç‰Œç”»åƒåˆ†æå·¥å…·

åŸºäºå•†å“æ ‡é¢˜ã€ä»·æ ¼ã€é”€é‡ç­‰æ–‡æœ¬ä¿¡æ¯ï¼Œä½¿ç”¨OpenAIå¤§æ¨¡å‹åˆ†æå„å“ç‰Œçš„å•†å“æ•°æ®
ç”Ÿæˆç®€æ´å®ç”¨çš„å“ç‰Œç”»åƒæŠ¥å‘Šï¼Œä¸ºè´­ç‰©è€…æä¾›å“ç‰Œæ¨èå’Œè´­ä¹°æŒ‡å¼•

ä½œè€…: AI Assistant
åˆ›å»ºæ—¶é—´: 2024-08-25
"""

import json
import os
import pathlib
import sys
import statistics
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
import time
import re
import glob


project_root = pathlib.Path(__file__).resolve().parents[4]
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'mall_rag_admin.settings')

# from utils.dj_setup import *

from dotenv import load_dotenv

load_dotenv()
place_id = os.environ.get('PLACE_ID')
monthly_type = os.environ.get('MONTHLY_TYPE')

class BrandAnalyzer:
    """å“ç‰Œå•†å“æ•°æ®åˆ†æå™¨ - åŸºäºæ–‡æœ¬ä¿¡æ¯ç”Ÿæˆå“ç‰Œç”»åƒ"""
    
    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, date_dir: str = "202508", force_reanalyze: bool = False):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡OPENAI_API_KEYè·å–
            base_url: APIåŸºç¡€URLï¼Œæ”¯æŒè‡ªå®šä¹‰ç«¯ç‚¹
            date_dir: æ—¥æœŸç›®å½•åç§°ï¼Œç”¨äºè¾“å‡ºæ–‡ä»¶è·¯å¾„
            force_reanalyze: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ†ææ‰€æœ‰å“ç‰Œ
        """
        # è®¾ç½®APIå¯†é’¥å’ŒåŸºç¡€URL
        self.api_key = api_key or "2a62f511-c4ac-4415-b51f-080931703a0b"
        self.base_url = base_url or "https://ark.cn-beijing.volces.com/api/v3"
        
        # åˆå§‹åŒ–è±†åŒ…thinkingæ¨¡å‹
        self.client = ChatOpenAI(
            temperature=0.7,
            model="doubao-seed-1-6-thinking-250715",
            openai_api_key=self.api_key,
            openai_api_base=self.base_url,
            max_tokens=6000,  # é€‚ä¸­çš„è¾“å‡ºé•¿åº¦
            streaming=True
        )
        
        # å“ç‰Œæ•°æ®å­˜å‚¨
        self.brand_data = {}
        self.analysis_results = {}
        self.date_dir = pathlib.Path(date_dir)
        self.force_reanalyze = force_reanalyze
        
    def check_brand_already_analyzed(self, brand: str) -> bool:
        """
        æ£€æŸ¥å“ç‰Œæ˜¯å¦å·²ç»åˆ†æè¿‡
        
        Args:
            brand: å“ç‰Œåç§°
            
        Returns:
            Trueè¡¨ç¤ºå·²åˆ†æè¿‡ï¼ŒFalseè¡¨ç¤ºæœªåˆ†æ
        """
        # å¦‚æœå¼ºåˆ¶é‡æ–°åˆ†æï¼Œç›´æ¥è¿”å›False
        if self.force_reanalyze:
            print(f"ğŸ”„ {brand} å“ç‰Œå¼ºåˆ¶é‡æ–°åˆ†æ")
            return False
            
        try:
            # æ£€æŸ¥å“ç‰Œåˆ†æç»“æœæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            safe_brand_name = brand.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
            latest_file = fr"data/{place_id}/{monthly_type}/brand_analysis/{brand}/{safe_brand_name}_latest.json"

            if os.path.exists(latest_file):
                # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦å®Œæ•´
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        existing_result = json.load(f)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„å“ç‰Œç”»åƒ
                    brand_profile = existing_result.get('brand_profile', {})
                    if isinstance(brand_profile, dict) and 'market_position' in brand_profile:
                        print(f"âœ… {brand} å“ç‰Œå·²åˆ†æè¿‡ï¼Œè·³è¿‡é‡å¤åˆ†æ")
                        return True
                    elif 'error' in brand_profile:
                        print(f"âš ï¸  {brand} å“ç‰Œä¹‹å‰åˆ†æå¤±è´¥ï¼Œé‡æ–°åˆ†æ")
                        return False
                    else:
                        print(f"âš ï¸  {brand} å“ç‰Œåˆ†æç»“æœä¸å®Œæ•´ï¼Œé‡æ–°åˆ†æ")
                        return False
                        
                except Exception as e:
                    print(f"âš ï¸  è¯»å– {brand} ç°æœ‰ç»“æœæ—¶å‡ºé”™: {str(e)}ï¼Œé‡æ–°åˆ†æ")
                    return False
            
            return False
            
        except Exception as e:
            print(f"âš ï¸  æ£€æŸ¥ {brand} åˆ†æçŠ¶æ€æ—¶å‡ºé”™: {str(e)}ï¼Œé‡æ–°åˆ†æ")
            return False
    
    def load_existing_analysis_results(self) -> Dict[str, Any]:
        """
        åŠ è½½å·²å­˜åœ¨çš„åˆ†æç»“æœ
        
        Returns:
            å·²å­˜åœ¨çš„åˆ†æç»“æœ
        """
        existing_results = {
            'analysis_time': datetime.now().isoformat(),
            'total_brands': 0,
            'brand_analyses': {},
            'skipped_brands': [],
            'existing_brands': []
        }
        
        brand_analysis_dir = self.date_dir/f"brand_analysis"
        if not os.path.exists(brand_analysis_dir):
            return existing_results
        
        # éå†å“ç‰Œåˆ†æç›®å½•
        for brand_dir in os.listdir(brand_analysis_dir):
            brand_path = os.path.join(brand_analysis_dir, brand_dir)
            if os.path.isdir(brand_path):
                # æŸ¥æ‰¾æœ€æ–°åˆ†ææ–‡ä»¶
                safe_brand_name = brand_dir.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
                latest_file = os.path.join(brand_path, f"{safe_brand_name}_latest.json")
                
                if os.path.exists(latest_file):
                    try:
                        with open(latest_file, 'r', encoding='utf-8') as f:
                            existing_result = json.load(f)
                        
                        # æ£€æŸ¥ç»“æœæ˜¯å¦å®Œæ•´
                        brand_profile = existing_result.get('brand_profile', {})
                        if isinstance(brand_profile, dict) and 'market_position' in brand_profile:
                            existing_results['brand_analyses'][brand_dir] = existing_result
                            existing_results['existing_brands'].append(brand_dir)
                            existing_results['total_brands'] += 1
                        else:
                            existing_results['skipped_brands'].append(brand_dir)
                            
                    except Exception as e:
                        print(f"âš ï¸  åŠ è½½ {brand_dir} ç°æœ‰ç»“æœæ—¶å‡ºé”™: {str(e)}")
                        existing_results['skipped_brands'].append(brand_dir)
        
        return existing_results
        
    def load_products_from_batch_directory(self, batch_dir: str) -> Dict[str, List[Dict]]:
        """
        ä»æ‰¹é‡äº§å“ç›®å½•ä¸­åŠ è½½æ‰€æœ‰å“ç‰Œçš„äº§å“æ•°æ®
        
        Args:
            batch_dir: æ‰¹é‡äº§å“ç›®å½•è·¯å¾„
            
        Returns:
            æŒ‰å“ç‰Œåˆ†ç»„çš„å•†å“æ•°æ®ï¼ŒåªåŒ…å«æ ¸å¿ƒæ–‡æœ¬å­—æ®µ
        """
        print(f"ğŸ“Š æ­£åœ¨ä»æ‰¹é‡äº§å“ç›®å½•åŠ è½½æ•°æ®: {batch_dir}")
        print("ğŸ¯ ä¸“æ³¨æ ¸å¿ƒæ–‡æœ¬ä¿¡æ¯ï¼šå•†å“æ ‡é¢˜ã€ä»·æ ¼ã€é”€é‡ã€å“ç‰Œç­‰")
        
        # æŸ¥æ‰¾æ‰€æœ‰äº§å“JSONæ–‡ä»¶
        product_files = glob.glob(os.path.join(batch_dir, "*_products.json"))
        
        if not product_files:
            print(f"âŒ é”™è¯¯: åœ¨ç›®å½• {batch_dir} ä¸­æ‰¾ä¸åˆ°äº§å“æ–‡ä»¶")
            return {}
        
        print(f"ğŸ“ æ‰¾åˆ° {len(product_files)} ä¸ªå“ç‰Œäº§å“æ–‡ä»¶")
        
        # æŒ‰å“ç‰Œåˆ†ç»„
        brand_groups = {}
        total_products = 0
        
        # å®šä¹‰éœ€è¦çš„æ ¸å¿ƒå­—æ®µ
        required_fields = {
            'product_id': 'å•†å“ID',
            'title': 'å•†å“æ ‡é¢˜', 
            'price': 'ä»·æ ¼',
            'sales_count': 'é”€é‡',
            'brand': 'å“ç‰Œ',
            'detail_url': 'è¯¦æƒ…é“¾æ¥'
        }
        
        for product_file in product_files:
            try:
                # ä»æ–‡ä»¶åæå–å“ç‰Œå
                filename = os.path.basename(product_file)
                brand_name = filename.replace("_products.json", "")
                
                print(f"ğŸ“– æ­£åœ¨åŠ è½½å“ç‰Œ {brand_name} çš„äº§å“æ•°æ®...")
                
                with open(product_file, 'r', encoding='utf-8') as f:
                    products = json.load(f)
                
                # è¿‡æ»¤å’Œæ¸…ç†äº§å“æ•°æ®ï¼Œåªä¿ç•™æ ¸å¿ƒæ–‡æœ¬å­—æ®µ
                filtered_products = []
                for product in products:
                    filtered_product = {}
                    
                    # åªä¿ç•™éœ€è¦çš„å­—æ®µ
                    for field, description in required_fields.items():
                        if field in product:
                            filtered_product[field] = product[field]
                        else:
                            # å¯¹äºç¼ºå¤±çš„å­—æ®µï¼Œè®¾ç½®é»˜è®¤å€¼
                            if field == 'price':
                                filtered_product[field] = "0.00"
                            elif field == 'sales_count':
                                filtered_product[field] = "0"
                            elif field == 'brand':
                                filtered_product[field] = brand_name
                            else:
                                filtered_product[field] = ""
                    
                    filtered_products.append(filtered_product)
                
                brand_groups[brand_name] = filtered_products
                total_products += len(filtered_products)
                
                print(f"âœ… {brand_name}: åŠ è½½äº† {len(filtered_products)} ä¸ªäº§å“ï¼ˆå·²è¿‡æ»¤æ ¸å¿ƒå­—æ®µï¼‰")
                
            except Exception as e:
                print(f"âŒ åŠ è½½å“ç‰Œ {filename} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        print(f"âœ… æˆåŠŸåŠ è½½ {total_products} ä¸ªå•†å“ï¼Œæ¶µç›– {len(brand_groups)} ä¸ªå“ç‰Œ")
        print("ğŸ“ æ•°æ®å­—æ®µï¼šå•†å“IDã€æ ‡é¢˜ã€ä»·æ ¼ã€é”€é‡ã€å“ç‰Œã€è¯¦æƒ…é“¾æ¥")
        
        self.brand_data = brand_groups
        return brand_groups
    
    def load_product_data(self, json_file: str) -> Dict[str, List[Dict]]:
        """
        åŠ è½½å•†å“æ•°æ®å¹¶æŒ‰å“ç‰Œåˆ†ç»„ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        
        Args:
            json_file: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            æŒ‰å“ç‰Œåˆ†ç»„çš„å•†å“æ•°æ®
        """
        print(f"ğŸ“Š æ­£åœ¨åŠ è½½å•†å“æ•°æ®: {json_file}")
        
        with open(json_file, 'r', encoding='utf-8') as f:
            products = json.load(f)
        
        # æŒ‰å“ç‰Œåˆ†ç»„
        brand_groups = {}
        for product in products:
            brand = product.get('brand', 'Unknown')
            if brand not in brand_groups:
                brand_groups[brand] = []
            brand_groups[brand].append(product)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(products)} ä¸ªå•†å“ï¼Œæ¶µç›– {len(brand_groups)} ä¸ªå“ç‰Œ")
        
        self.brand_data = brand_groups
        return brand_groups
    
    def calculate_brand_statistics(self, brand: str, products: List[Dict]) -> Dict[str, Any]:
        """
        åŸºäºæ–‡æœ¬ä¿¡æ¯è®¡ç®—å“ç‰Œç»Ÿè®¡æ•°æ® - ä½¿ç”¨æ‰€æœ‰å•†å“
        
        Args:
            brand: å“ç‰Œåç§°
            products: å•†å“åˆ—è¡¨
            
        Returns:
            å“ç‰Œç»Ÿè®¡æ•°æ®
        """
        # ä½¿ç”¨æ‰€æœ‰å•†å“è¿›è¡Œåˆ†æï¼Œä¸è¿›è¡Œé€‰æ‹©
        all_products = products
        
        stats = {
            'brand_name': brand,
            'total_products': len(products),
            'analyzed_products': len(all_products),  # åˆ†ææ‰€æœ‰å•†å“
            'price_stats': {},
            'sales_stats': {},
            'title_analysis': {},  # æ ‡é¢˜åˆ†æ
            'product_categories': {},
            'detailed_product_samples': [],  # æ‰€æœ‰å•†å“æ ·æœ¬
            'price_segments': {},  # ä»·æ ¼åŒºé—´åˆ†æ
            'sales_performance': {}  # é”€å”®è¡¨ç°åˆ†æ
        }
        
        # ä»·æ ¼ç»Ÿè®¡ - ä½¿ç”¨å…¨éƒ¨å•†å“æ•°æ®
        all_prices = []
        for p in products:
            price_str = p.get('price', '0')
            try:
                # å¤„ç†ä»·æ ¼å­—ç¬¦ä¸²ï¼Œæå–æ•°å­—éƒ¨åˆ†
                price = float(price_str.replace('Â¥', '').replace(',', '').strip())
                if price > 0:
                    all_prices.append(price)
            except (ValueError, AttributeError):
                continue
        
        if all_prices:
            stats['price_stats'] = {
                'min_price': min(all_prices),
                'max_price': max(all_prices),
                'avg_price': round(statistics.mean(all_prices), 2),
                'median_price': round(statistics.median(all_prices), 2),
                'price_range': max(all_prices) - min(all_prices),
                'has_price_count': len(all_prices),
                'price_coverage': round(len(all_prices) / len(products) * 100, 1)
            }
        
        # é”€é‡ç»Ÿè®¡
        all_sales = []
        for p in products:
            sales_str = p.get('sales_count', '0')
            try:
                # å¤„ç†é”€é‡å­—ç¬¦ä¸²ï¼Œæå–æ•°å­—éƒ¨åˆ†
                sales = int(sales_str.replace(',', '').strip())
                if sales > 0:
                    all_sales.append(sales)
            except (ValueError, AttributeError):
                continue
        
        if all_sales:
            stats['sales_stats'] = {
                'min_sales': min(all_sales),
                'max_sales': max(all_sales),
                'avg_sales': round(statistics.mean(all_sales), 2),
                'median_sales': round(statistics.median(all_sales), 2),
                'has_sales_count': len(all_sales),
                'sales_coverage': round(len(all_sales) / len(products) * 100, 1)
            }
        
        # æ ‡é¢˜åˆ†æ - åŸºäºæ‰€æœ‰å•†å“
        all_titles = [p.get('title', '') for p in products if p.get('title')]
        
        # å…³é”®è¯åˆ†æ
        keywords = {}
        common_words = ['æ–°æ¬¾', 'æ—¶å°š', 'ç»å…¸', 'ç®€çº¦', 'æ½®æµ', 'ç™¾æ­', 'èˆ’é€‚', 'å®˜æ–¹', 'æ­£å“', 
                       'ç”·æ¬¾', 'å¥³æ¬¾', 'æƒ…ä¾£', 'å•†åŠ¡', 'ä¼‘é—²', 'è¿åŠ¨', 'é«˜ç«¯', 'å¥¢å', 'ç²¾è‡´',
                       'æ–°å“', 'çƒ­é”€', 'çˆ†æ¬¾', 'æ¨è', 'ç²¾é€‰', 'é™é‡', 'ç‰¹ä»·', 'ä¿ƒé”€']
        
        for word in common_words:
            count = sum(1 for title in all_titles if word in title)
            if count > 0:
                keywords[word] = {
                    'count': count,
                    'percentage': round(count / len(all_titles) * 100, 1)
                }
        
        stats['title_analysis'] = {
            'keywords': keywords,
            'sample_titles': all_titles[:20],  # å‰20ä¸ªæ ‡é¢˜ä½œä¸ºæ ·æœ¬
            'title_length_stats': {
                'avg_length': round(sum(len(t) for t in all_titles) / len(all_titles), 1) if all_titles else 0,
                'max_length': max(len(t) for t in all_titles) if all_titles else 0,
                'min_length': min(len(t) for t in all_titles) if all_titles else 0
            }
        }
        
        # æ‰€æœ‰å•†å“æ ·æœ¬ - åŒ…å«æ ¸å¿ƒä¿¡æ¯
        detailed_samples = []
        for i, p in enumerate(all_products):
            sample = {
                'index': i + 1,
                'product_id': p.get('product_id', ''),
                'title': p.get('title', '')[:100],  # ç¼©çŸ­æ ‡é¢˜é•¿åº¦ä»¥èŠ‚çœç©ºé—´
                'price': p.get('price', ''),
                'price_numeric': p.get('price_numeric', 0),
                'sales_volume': p.get('sales_volume', ''),
                'sales_numeric': p.get('sales_numeric', 0),
                'product_url': p.get('product_url', '')
            }
            detailed_samples.append(sample)
        stats['detailed_product_samples'] = detailed_samples
        
        # ä»·æ ¼åŒºé—´åˆ†æ - åŸºäºå…¨éƒ¨å•†å“æ•°æ®
        if all_prices:
            try:
                median_price = statistics.median(all_prices)
                price_ranges = {
                    'low': [p for p in all_prices if p < median_price * 0.7],
                    'medium': [p for p in all_prices if median_price * 0.7 <= p <= median_price * 1.3],
                    'high': [p for p in all_prices if p > median_price * 1.3]
                }
                
                stats['price_segments'] = {
                    'low_price': {
                        'count': len(price_ranges['low']),
                        'percentage': round(len(price_ranges['low']) / len(all_prices) * 100, 1),
                        'avg_price': round(statistics.mean(price_ranges['low']), 2) if price_ranges['low'] else 0
                    },
                    'medium_price': {
                        'count': len(price_ranges['medium']),
                        'percentage': round(len(price_ranges['medium']) / len(all_prices) * 100, 1),
                        'avg_price': round(statistics.mean(price_ranges['medium']), 2) if price_ranges['medium'] else 0
                    },
                    'high_price': {
                        'count': len(price_ranges['high']),
                        'percentage': round(len(price_ranges['high']) / len(all_prices) * 100, 1),
                        'avg_price': round(statistics.mean(price_ranges['high']), 2) if price_ranges['high'] else 0
                    }
                }
            except Exception as e:
                print(f"âš ï¸  ä»·æ ¼åŒºé—´åˆ†æå‡ºé”™: {str(e)}")
                stats['price_segments'] = {}
        
        # é”€å”®è¡¨ç°åˆ†æ
        if all_sales:
            try:
                sales_median = statistics.median(all_sales)
                hot_products = []
                cold_products = []
                
                for p in products:
                    sales_num = p.get('sales_numeric')
                    if sales_num is not None and isinstance(sales_num, (int, float)):
                        if sales_num > sales_median * 1.5:
                            hot_products.append(p)
                        elif sales_num < sales_median * 0.5:
                            cold_products.append(p)
                
                stats['sales_performance'] = {
                    'hot_products_count': len(hot_products),
                    'hot_products_percentage': round(len(hot_products) / len(products) * 100, 1),
                    'cold_products_count': len(cold_products),
                    'cold_products_percentage': round(len(cold_products) / len(products) * 100, 1),
                    'hot_product_samples': [p.get('title', '')[:80] for p in hot_products[:5]],  # å¢åŠ çƒ­é”€å•†å“æ ·æœ¬
                    'sales_median': sales_median
                }
            except Exception as e:
                print(f"âš ï¸  é”€å”®è¡¨ç°åˆ†æå‡ºé”™: {str(e)}")
                stats['sales_performance'] = {}
        
        return stats
    
    def generate_brand_analysis_prompt(self, brand_stats: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆå“ç‰Œç”»åƒåˆ†æçš„æç¤ºè¯ - åŸºäºæ‰€æœ‰å•†å“æ–‡æœ¬ä¿¡æ¯
        
        Args:
            brand_stats: å“ç‰Œç»Ÿè®¡æ•°æ®
            
        Returns:
            åˆ†ææç¤ºè¯
        """
        # æå–æ ¸å¿ƒä¿¡æ¯ç”¨äºåˆ†æ
        price_info = brand_stats.get('price_stats', {})
        sales_info = brand_stats.get('sales_stats', {})
        title_info = brand_stats.get('title_analysis', {})
        all_products = brand_stats.get('detailed_product_samples', [])  # æ‰€æœ‰å•†å“
        
        # å¦‚æœå•†å“æ•°é‡è¿‡å¤šï¼Œè¿›è¡Œæ™ºèƒ½åˆ†ç»„å±•ç¤º
        total_products = len(all_products)
        if total_products > 50:
            # å•†å“æ•°é‡è¿‡å¤šæ—¶ï¼ŒæŒ‰ä»·æ ¼æ®µåˆ†ç»„å±•ç¤º
            try:
                median_price = price_info.get('median_price', 1000)
                if median_price and isinstance(median_price, (int, float)) and median_price > 0:
                    low_price_products = [p for p in all_products if p.get('price_numeric', 0) and isinstance(p.get('price_numeric'), (int, float)) and p.get('price_numeric', 0) < median_price * 0.7]
                    medium_price_products = [p for p in all_products if p.get('price_numeric', 0) and isinstance(p.get('price_numeric'), (int, float)) and median_price * 0.7 <= p.get('price_numeric', 0) <= median_price * 1.3]
                    high_price_products = [p for p in all_products if p.get('price_numeric', 0) and isinstance(p.get('price_numeric'), (int, float)) and p.get('price_numeric', 0) > median_price * 1.3]
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ä¸­ä½æ•°ä»·æ ¼ï¼ŒæŒ‰ä»·æ ¼æ’åºåˆ†ç»„
                    valid_prices = [p for p in all_products if p.get('price_numeric') and isinstance(p.get('price_numeric'), (int, float)) and p.get('price_numeric', 0) > 0]
                    if valid_prices:
                        sorted_prices = sorted(valid_prices, key=lambda x: x.get('price_numeric', 0))
                        split_point = len(sorted_prices) // 3
                        low_price_products = sorted_prices[:split_point]
                        medium_price_products = sorted_prices[split_point:split_point*2]
                        high_price_products = sorted_prices[split_point*2:]
                    else:
                        low_price_products = []
                        medium_price_products = []
                        high_price_products = []
                
                # æ¯ä¸ªä»·æ ¼æ®µé€‰æ‹©ä»£è¡¨æ€§å•†å“
                sample_products = {
                    'low_price_samples': low_price_products[:10] if len(low_price_products) > 10 else low_price_products,
                    'medium_price_samples': medium_price_products[:15] if len(medium_price_products) > 15 else medium_price_products,
                    'high_price_samples': high_price_products[:10] if len(high_price_products) > 10 else high_price_products
                }
                
                products_display = f"""
## å•†å“æ•°æ®æ¦‚è§ˆï¼ˆå…±{total_products}ä¸ªå•†å“ï¼‰
- ä½ä»·å•†å“ï¼ˆ{len(low_price_products)}ä¸ªï¼‰: {len(sample_products['low_price_samples'])}ä¸ªæ ·æœ¬
- ä¸­ä»·å•†å“ï¼ˆ{len(medium_price_products)}ä¸ªï¼‰: {len(sample_products['medium_price_samples'])}ä¸ªæ ·æœ¬  
- é«˜ä»·å•†å“ï¼ˆ{len(high_price_products)}ä¸ªï¼‰: {len(sample_products['high_price_samples'])}ä¸ªæ ·æœ¬

## å„ä»·æ ¼æ®µå•†å“æ ·æœ¬
### ä½ä»·å•†å“æ ·æœ¬
{json.dumps(sample_products['low_price_samples'], ensure_ascii=False, indent=2)}

### ä¸­ä»·å•†å“æ ·æœ¬
{json.dumps(sample_products['medium_price_samples'], ensure_ascii=False, indent=2)}

### é«˜ä»·å•†å“æ ·æœ¬
{json.dumps(sample_products['high_price_samples'], ensure_ascii=False, indent=2)}
"""
            except Exception as e:
                print(f"âš ï¸  ä»·æ ¼åˆ†ç»„å‡ºé”™: {str(e)}")
                # å¦‚æœåˆ†ç»„å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å‰Nä¸ªå•†å“
                products_display = f"""
## å•†å“æ•°æ®æ¦‚è§ˆï¼ˆå…±{total_products}ä¸ªå•†å“ï¼‰
- ç”±äºä»·æ ¼åˆ†ç»„å‡ºé”™ï¼Œå±•ç¤ºå‰50ä¸ªå•†å“æ ·æœ¬

## å•†å“æ ·æœ¬
{json.dumps(all_products[:50], ensure_ascii=False, indent=2)}
"""
        else:
            # å•†å“æ•°é‡é€‚ä¸­æ—¶ï¼Œå±•ç¤ºæ‰€æœ‰å•†å“
            products_display = f"""
## æ‰€æœ‰å•†å“æ•°æ®ï¼ˆå…±{total_products}ä¸ªï¼‰
{json.dumps(all_products, ensure_ascii=False, indent=2)}
"""
        
        prompt = f"""
è¯·åŸºäºä»¥ä¸‹å®Œæ•´çš„å•†å“æ•°æ®ï¼Œä¸º"{brand_stats['brand_name']}"å“ç‰Œç”Ÿæˆå…¨é¢å‡†ç¡®çš„å“ç‰Œç”»åƒã€‚

**é‡è¦æç¤ºï¼šè¿™äº›å•†å“éƒ½æ˜¯è¿‘æœŸä¸Šæ¶çš„æ–°å“ï¼Œä»£è¡¨äº†å“ç‰Œæœ€æ–°çš„äº§å“ç­–ç•¥å’Œå¸‚åœºå®šä½ã€‚**

## å“ç‰ŒåŸºç¡€ä¿¡æ¯
- å“ç‰Œåç§°: {brand_stats['brand_name']}
- å•†å“æ€»æ•°: {brand_stats['total_products']}ä¸ªï¼ˆå…¨éƒ¨ä¸ºæ–°å“ï¼‰
- åˆ†æå•†å“æ•°: {brand_stats['analyzed_products']}ä¸ª
- ä»·æ ¼èŒƒå›´: Â¥{price_info.get('min_price', 0):.0f} - Â¥{price_info.get('max_price', 0):.0f}
- å¹³å‡ä»·æ ¼: Â¥{price_info.get('avg_price', 0):.0f}
- ä¸­ä½æ•°ä»·æ ¼: Â¥{price_info.get('median_price', 0):.0f}

## é”€é‡è¡¨ç°
- å¹³å‡é”€é‡: {sales_info.get('avg_sales', 0):.0f}
- æœ€é«˜é”€é‡: {sales_info.get('max_sales', 0):.0f}
- é”€é‡è¦†ç›–: {sales_info.get('sales_coverage', 0):.1f}%

## æ ‡é¢˜å…³é”®è¯åˆ†æ
{json.dumps(title_info.get('keywords', {}), ensure_ascii=False, indent=2)}

## ä»·æ ¼åŒºé—´åˆ†å¸ƒ
{json.dumps(brand_stats.get('price_segments', {}), ensure_ascii=False, indent=2)}

## é”€å”®è¡¨ç°åˆ†æ
{json.dumps(brand_stats.get('sales_performance', {}), ensure_ascii=False, indent=2)}

{products_display}

è¯·åŸºäºä»¥ä¸Šå®Œæ•´çš„å•†å“æ•°æ®ï¼ˆæ ‡é¢˜ã€ä»·æ ¼ã€é”€é‡ï¼‰ï¼Œç”Ÿæˆå…¨é¢å‡†ç¡®çš„å“ç‰Œç”»åƒJSONï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒå†…å®¹ï¼š

```json
{{
  "brand_name": "{brand_stats['brand_name']}",
  "product_style": "äº§å“é£æ ¼ç‰¹ç‚¹",
  "brand_category": "å“ç‰Œç±»åˆ«ï¼ˆå¦‚ï¼šç‘å£«æ‰‹è¡¨å“ç‰Œã€è¿åŠ¨é‹å“ç‰Œç­‰ï¼‰",
  "price_positioning": "ä»·æ ¼å®šä½ï¼ˆå¦‚ï¼šä¸­é«˜ç«¯ã€æ€§ä»·æ¯”ã€å¥¢åç­‰ï¼‰",
  "market_position": "å¸‚åœºå®šä½ï¼ˆä¸€å¥è¯æ¦‚æ‹¬å“ç‰Œåœ¨å¸‚åœºä¸­çš„ä½ç½®ï¼‰",
  "target_users": "ç›®æ ‡ç”¨æˆ·ç¾¤ä½“ï¼ˆå¹´é¾„ã€æ”¶å…¥ã€ç”Ÿæ´»æ–¹å¼ç­‰ï¼‰",
  "price_strategy": "ä»·æ ¼ç­–ç•¥åˆ†æ",
  "sales_characteristics": "é”€å”®ç‰¹å¾åˆ†æ",
  "core_features": ["æ ¸å¿ƒç‰¹è‰²1", "æ ¸å¿ƒç‰¹è‰²2", "æ ¸å¿ƒç‰¹è‰²3"],
  "buying_advice": ["æœ€ä½³è´­ä¹°ä»·æ ¼æ®µ", "é€‰è´­è¦ç‚¹", "é¿å‘æé†’", "æ–°å“è´­ä¹°æ³¨æ„äº‹é¡¹"],
  "recommended_scenarios": ["æ¨èä½¿ç”¨åœºæ™¯1", "æ¨èä½¿ç”¨åœºæ™¯2", "æ¨èä½¿ç”¨åœºæ™¯3"],
  "new_product_strategy": ["æ–°å“ç­–ç•¥é‡ç‚¹", "æ–°å“è¶‹åŠ¿æ–¹å‘", "åˆ›æ–°ç‰¹è‰²", "å¸‚åœºååº”åˆ†æ"],
  "brand_keywords": ["æœç´¢å…³é”®è¯1", "æœç´¢å…³é”®è¯2", "æœç´¢å…³é”®è¯3", "æœç´¢å…³é”®è¯4", "æœç´¢å…³é”®è¯5"]
}}
```

è¦æ±‚ï¼š
1. åŸºäºæ‰€æœ‰{total_products}ä¸ªæ–°å“çš„å®Œæ•´æ•°æ®è¿›è¡Œåˆ†æ
2. å†…å®¹å…¨é¢å‡†ç¡®ï¼Œé¿å…ç©ºæ³›æè¿°
3. åŸºäºå•†å“æ ‡é¢˜ã€ä»·æ ¼ã€é”€é‡ç­‰æ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ·±å…¥åˆ†æ
4. æ‰€æœ‰å†…å®¹éƒ½æ˜¯ä¸­æ–‡
5. é‡ç‚¹çªå‡ºå“ç‰Œæ–°å“ç­–ç•¥ã€åˆ›æ–°ç‰¹è‰²å’Œå¸‚åœºè¶‹åŠ¿
6. é€‚åˆRAGæ£€ç´¢å’Œè´­ç‰©æ¨è
7. åˆ†æè¦æ·±å…¥ï¼Œä½“ç°æ–°å“æ•°æ®åˆ†æçš„ä¼˜åŠ¿
8. å¼ºè°ƒè¿™äº›å•†å“éƒ½æ˜¯æ–°å“ï¼Œåˆ†æå“ç‰Œçš„æ–°å“ç­–ç•¥å’Œå¸‚åœºå®šä½
"""
        return prompt
    
    def analyze_brand_with_ai(self, brand: str, brand_stats: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨AIåˆ†æå“ç‰Œ
        
        Args:
            brand: å“ç‰Œåç§°
            brand_stats: å“ç‰Œç»Ÿè®¡æ•°æ®
            
        Returns:
            AIåˆ†æç»“æœ
        """
        print(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨AIåˆ†æå“ç‰Œ: {brand}")
        
        try:
            prompt = self.generate_brand_analysis_prompt(brand_stats)
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç”µå•†æ•°æ®åˆ†æå¸ˆå’Œå“ç‰Œç­–ç•¥ä¸“å®¶ï¼Œæ“…é•¿ä»æ–°å“å•†å“æ•°æ®ä¸­æ´å¯Ÿå“ç‰Œçš„æ–°å“ç­–ç•¥ã€åˆ›æ–°ç‰¹è‰²å’Œå¸‚åœºè¶‹åŠ¿ã€‚è¯·åŸºäºæ‰€æœ‰æ–°å“çš„æ ‡é¢˜ã€ä»·æ ¼ã€é”€é‡ç­‰æ–‡æœ¬ä¿¡æ¯ï¼Œæä¾›æ·±å…¥ã€ä¸“ä¸šçš„å“ç‰Œç”»åƒåˆ†æã€‚"),
                HumanMessage(content=prompt)
            ]
            
            # è°ƒç”¨è±†åŒ…thinkingæ¨¡å‹
            print(f"ğŸ§  å¼€å§‹thinkingåˆ†æå“ç‰Œ: {brand}")
            print("=" * 60)
            
            # ä½¿ç”¨æµå¼è¾“å‡ºæ¥å®æ—¶æ˜¾ç¤ºthinkingè¿‡ç¨‹
            if hasattr(self.client, 'stream') and self.client.streaming:
                print("ğŸ’­ å®æ—¶thinkingè¿‡ç¨‹:")
                print("-" * 40)
                
                full_response = ""
                try:
                    # ä½¿ç”¨streamæ–¹æ³•è·å–æµå¼å“åº”
                    for chunk in self.client.stream(messages):
                        if hasattr(chunk, 'content') and chunk.content:
                            content = chunk.content
                            print(content, end='', flush=True)
                            full_response += content
                    
                    ai_response = full_response
                    print("\n" + "=" * 60)
                except Exception as e:
                    print(f"\nâš ï¸  æµå¼è¾“å‡ºå‡ºé”™ï¼Œåˆ‡æ¢åˆ°æ™®é€šæ¨¡å¼: {str(e)}")
                    # å¦‚æœæµå¼è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼
                    response = self.client.invoke(messages)
                    ai_response = response.content
                    
                    # æ‰“å°thinkingè¿‡ç¨‹
                    print("ğŸ¤” AIåˆ†æè¿‡ç¨‹:")
                    print("-" * 40)
                    
                    # å¦‚æœå“åº”åŒ…å«thinkingæ ‡è®°ï¼Œåˆ†ç¦»thinkingå’Œæœ€ç»ˆç­”æ¡ˆ
                    if "<thinking>" in ai_response and "</thinking>" in ai_response:
                        # æå–thinkingéƒ¨åˆ†
                        thinking_start = ai_response.find("<thinking>") + len("<thinking>")
                        thinking_end = ai_response.find("</thinking>")
                        thinking_content = ai_response[thinking_start:thinking_end].strip()
                        
                        # æå–æœ€ç»ˆç­”æ¡ˆéƒ¨åˆ†
                        final_answer = ai_response[thinking_end + len("</thinking>"):].strip()
                        
                        print("ğŸ’­ æ€è€ƒè¿‡ç¨‹:")
                        print(thinking_content[:2000] + "..." if len(thinking_content) > 2000 else thinking_content)
                        print("\n" + "-"*40)
                        print("ğŸ“ æœ€ç»ˆåˆ†æç»“æœ:")
                        print(final_answer[:1000] + "..." if len(final_answer) > 1000 else final_answer)
                        
                        # ä½¿ç”¨æœ€ç»ˆç­”æ¡ˆè¿›è¡Œåç»­å¤„ç†
                        ai_response = final_answer
                    else:
                        print("ğŸ“„ å®Œæ•´å“åº”:")
                        print(ai_response[:1500] + "..." if len(ai_response) > 1500 else ai_response)
                    
                    print("=" * 60)
            else:
                response = self.client.invoke(messages)
                ai_response = response.content
                
                # æ‰“å°thinkingè¿‡ç¨‹
                print("ğŸ¤” AIåˆ†æè¿‡ç¨‹:")
                print("-" * 40)
                
                # å¦‚æœå“åº”åŒ…å«thinkingæ ‡è®°ï¼Œåˆ†ç¦»thinkingå’Œæœ€ç»ˆç­”æ¡ˆ
                if "<thinking>" in ai_response and "</thinking>" in ai_response:
                    # æå–thinkingéƒ¨åˆ†
                    thinking_start = ai_response.find("<thinking>") + len("<thinking>")
                    thinking_end = ai_response.find("</thinking>")
                    thinking_content = ai_response[thinking_start:thinking_end].strip()
                    
                    # æå–æœ€ç»ˆç­”æ¡ˆéƒ¨åˆ†
                    final_answer = ai_response[thinking_end + len("</thinking>"):].strip()
                    
                    print("ğŸ’­ æ€è€ƒè¿‡ç¨‹:")
                    print(thinking_content[:2000] + "..." if len(thinking_content) > 2000 else thinking_content)
                    print("\n" + "-"*40)
                    print("ğŸ“ æœ€ç»ˆåˆ†æç»“æœ:")
                    print(final_answer[:1000] + "..." if len(final_answer) > 1000 else final_answer)
                    
                    # ä½¿ç”¨æœ€ç»ˆç­”æ¡ˆè¿›è¡Œåç»­å¤„ç†
                    ai_response = final_answer
                else:
                    print("ğŸ“„ å®Œæ•´å“åº”:")
                    print(ai_response[:1500] + "..." if len(ai_response) > 1500 else ai_response)
                
                print("=" * 60)
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤markdownæ ‡è®°
                cleaned_response = ai_response.strip()
                
                # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                if cleaned_response.startswith('```json'):
                    cleaned_response = cleaned_response[7:]  # ç§»é™¤ ```json
                if cleaned_response.startswith('```'):
                    cleaned_response = cleaned_response[3:]   # ç§»é™¤ ```
                if cleaned_response.endswith('```'):
                    cleaned_response = cleaned_response[:-3]  # ç§»é™¤ç»“å°¾çš„ ```
                
                cleaned_response = cleaned_response.strip()
                
                # å°è¯•æ‰¾åˆ°å®Œæ•´çš„JSONç»“æ„
                json_start = cleaned_response.find('{')
                json_end = cleaned_response.rfind('}')
                
                if json_start != -1 and json_end != -1 and json_end > json_start:
                    json_str = cleaned_response[json_start:json_end+1]
                    
                    # æ£€æŸ¥JSONæ˜¯å¦çœ‹èµ·æ¥å®Œæ•´
                    if '"brand_name"' in json_str and '"market_position"' in json_str:
                        try:
                            analysis_result = json.loads(json_str)
                            print(f"âœ… æˆåŠŸè§£æAIåˆ†æç»“æœJSON")
                        except json.JSONDecodeError as e:
                            print(f"âš ï¸  JSONæ ¼å¼æœ‰è¯¯: {str(e)}")
                            analysis_result = {
                                "raw_analysis": ai_response,
                                "parsing_note": "JSONæ ¼å¼é”™è¯¯ï¼Œå·²ä¿å­˜ä¸ºåŸå§‹æ–‡æœ¬"
                            }
                    else:
                        print(f"âš ï¸  AIè¿”å›çš„JSONç»“æ„ä¸å®Œæ•´")
                        analysis_result = {
                            "raw_analysis": ai_response,
                            "parsing_note": "AIè¿”å›çš„JSONä¸å®Œæ•´ï¼Œå·²ä¿å­˜ä¸ºåŸå§‹æ–‡æœ¬"
                        }
                else:
                    analysis_result = {
                        "raw_analysis": ai_response,
                        "parsing_note": "æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONç»“æ„ï¼Œå·²ä¿å­˜ä¸ºåŸå§‹æ–‡æœ¬"
                    }
                    
            except Exception as e:
                print(f"âš ï¸  JSONå¤„ç†å¼‚å¸¸: {str(e)}")
                analysis_result = {
                    "raw_analysis": ai_response,
                    "parsing_note": f"JSONå¤„ç†å¼‚å¸¸: {str(e)}ï¼Œå·²ä¿å­˜ä¸ºåŸå§‹æ–‡æœ¬"
                }
            
            print(f"âœ… {brand} å“ç‰Œåˆ†æå®Œæˆ")
            return analysis_result
            
        except Exception as e:
            print(f"âŒ {brand} å“ç‰Œåˆ†æå¤±è´¥: {str(e)}")
            return {
                "error": str(e),
                "brand": brand,
                "analysis_failed": True
            }
    
    def analyze_all_brands(self) -> Dict[str, Any]:
        """
        åˆ†ææ‰€æœ‰å“ç‰Œï¼Œè·³è¿‡å·²åˆ†æçš„å“ç‰Œ
        
        Returns:
            æ‰€æœ‰å“ç‰Œçš„åˆ†æç»“æœ
        """
        if not self.brand_data:
            raise ValueError("è¯·å…ˆåŠ è½½å•†å“æ•°æ®")
        
        print(f"ğŸš€ å¼€å§‹åˆ†æ {len(self.brand_data)} ä¸ªå“ç‰Œ...")
        
        # åŠ è½½å·²å­˜åœ¨çš„åˆ†æç»“æœ
        existing_results = self.load_existing_analysis_results()
        print(f"ğŸ“‹ å‘ç°å·²åˆ†æå“ç‰Œ: {len(existing_results['existing_brands'])} ä¸ª")
        if existing_results['skipped_brands']:
            print(f"âš ï¸  éœ€è¦é‡æ–°åˆ†æçš„å“ç‰Œ: {len(existing_results['skipped_brands'])} ä¸ª")
        
        results = {
            'analysis_time': datetime.now().isoformat(),
            'total_brands': len(self.brand_data),
            'brand_analyses': {},
            'skipped_brands': existing_results['skipped_brands'],
            'existing_brands': existing_results['existing_brands']
        }
        
        # å…ˆæ·»åŠ å·²å­˜åœ¨çš„åˆ†æç»“æœ
        results['brand_analyses'].update(existing_results['brand_analyses'])
        
        # åˆ†ææ–°å“ç‰Œæˆ–éœ€è¦é‡æ–°åˆ†æçš„å“ç‰Œ
        new_brands_count = 0
        reanalyzed_brands_count = 0
        
        for brand, products in self.brand_data.items():
            # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ†æè¿‡
            if self.check_brand_already_analyzed(brand):
                continue
            
            print(f"\nğŸ“Š åˆ†æå“ç‰Œ: {brand} ({len(products)} ä¸ªå•†å“)")
            
            try:
                # è®¡ç®—åŸºç¡€ç»Ÿè®¡
                brand_stats = self.calculate_brand_statistics(brand, products)
                
                # AIåˆ†æ
                ai_analysis = self.analyze_brand_with_ai(brand, brand_stats)
                
                # åˆå¹¶ç»“æœ
                brand_result = {
                    'brand_profile': ai_analysis,  # å“ç‰Œç”»åƒ
                    'summary_stats': {  # å…³é”®ç»Ÿè®¡ä¿¡æ¯
                        'total_products': brand_stats['total_products'],
                        'analyzed_products': brand_stats.get('analyzed_products', 0),
                        'price_range': {
                            'min': brand_stats.get('price_stats', {}).get('min_price', 0),
                            'max': brand_stats.get('price_stats', {}).get('max_price', 0),
                            'avg': brand_stats.get('price_stats', {}).get('avg_price', 0)
                        },
                        'sales_performance': {
                            'avg_sales': brand_stats.get('sales_stats', {}).get('avg_sales', 0),
                            'max_sales': brand_stats.get('sales_stats', {}).get('max_sales', 0)
                        }
                    },
                    'last_updated': datetime.now().isoformat()
                }
                
                results['brand_analyses'][brand] = brand_result
                
                # ç«‹å³ä¿å­˜å•ä¸ªå“ç‰Œç»“æœåˆ°ç‹¬ç«‹æ–‡ä»¶
                self.save_single_brand_result(brand, brand_result)
                
                # ç»Ÿè®¡æ–°åˆ†ææˆ–é‡æ–°åˆ†æçš„å“ç‰Œ
                if brand in existing_results['skipped_brands']:
                    reanalyzed_brands_count += 1
                    print(f"âœ… {brand} å“ç‰Œé‡æ–°åˆ†æå®Œæˆ")
                else:
                    new_brands_count += 1
                    print(f"âœ… {brand} å“ç‰Œæ–°åˆ†æå®Œæˆ")
                
            except Exception as e:
                print(f"âŒ {brand} å“ç‰Œåˆ†æå¤±è´¥: {str(e)}")
                # è®°å½•é”™è¯¯ä¿¡æ¯ï¼Œä½†ä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                error_result = {
                    'brand_profile': {
                        "error": str(e),
                        "brand": brand,
                        "analysis_failed": True
                    },
                    'summary_stats': {
                        'total_products': len(products),
                        'analyzed_products': 0,
                        'error_note': f"åˆ†æå¤±è´¥: {str(e)}"
                    },
                    'last_updated': datetime.now().isoformat()
                }
                
                results['brand_analyses'][brand] = error_result
                
                # å³ä½¿åˆ†æå¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ç»“æœ
                self.save_single_brand_result(brand, error_result)
            
            # é¿å…APIé™æµ
            time.sleep(1)
        
        # æ‰“å°åˆ†æç»Ÿè®¡
        total_analyzed = len(results['brand_analyses'])
        print(f"\nğŸ“Š åˆ†æå®Œæˆç»Ÿè®¡:")
        print(f"   - æ€»å“ç‰Œæ•°: {len(self.brand_data)}")
        print(f"   - å·²å­˜åœ¨ç»“æœ: {len(existing_results['existing_brands'])}")
        print(f"   - æ–°åˆ†æå“ç‰Œ: {new_brands_count}")
        print(f"   - é‡æ–°åˆ†æå“ç‰Œ: {reanalyzed_brands_count}")
        print(f"   - æœ¬æ¬¡å®é™…åˆ†æ: {new_brands_count + reanalyzed_brands_count}")
        print(f"   - æœ€ç»ˆç»“æœæ€»æ•°: {total_analyzed}")
        
        self.analysis_results = results
        return results
    
    def save_single_brand_result(self, brand: str, brand_result: Dict[str, Any]) -> None:
        """
        ä¿å­˜å•ä¸ªå“ç‰Œçš„åˆ†æç»“æœåˆ°ç‹¬ç«‹æ–‡ä»¶
        
        Args:
            brand: å“ç‰Œåç§°
            brand_result: å“ç‰Œåˆ†æç»“æœ
        """
        try:
            # åˆ›å»ºå“ç‰Œä¸“å±è¾“å‡ºç›®å½•ï¼Œæ”¾åœ¨æŒ‡å®šæ—¥æœŸç›®å½•ä¸‹
            brand_output_dir = self.date_dir/f"brand_analysis/{brand}"
            os.makedirs(brand_output_dir, exist_ok=True)
            
            # ç”Ÿæˆå“ç‰Œä¸“å±æ–‡ä»¶åï¼ˆåªä¿å­˜æœ€æ–°ç‰ˆæœ¬ï¼Œä¸å¸¦æ—¶é—´æˆ³ï¼‰
            safe_brand_name = brand.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
            latest_file = f"{brand_output_dir}/{safe_brand_name}_latest.json"
            
            # ä¿å­˜å“ç‰Œåˆ†æç»“æœï¼ˆåªä¿å­˜æœ€æ–°ç‰ˆæœ¬ï¼‰
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(brand_result, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ {brand} å“ç‰Œç»“æœå·²ä¿å­˜åˆ°: {latest_file}")
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ {brand} å“ç‰Œç»“æœå¤±è´¥: {str(e)}")
    
    def save_analysis_results(self, output_file: str = None) -> str:
        """
        ä¿å­˜åˆ†æç»“æœ
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not self.analysis_results:
            raise ValueError("æ²¡æœ‰åˆ†æç»“æœå¯ä¿å­˜")
        
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"brand_analysis_report_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def generate_summary_report(self) -> str:
        """
        ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        
        Returns:
            æ‘˜è¦æŠ¥å‘Šæ–‡æœ¬
        """
        if not self.analysis_results:
            return "æ²¡æœ‰åˆ†æç»“æœå¯ç”ŸæˆæŠ¥å‘Š"
        
        report = []
        report.append("=" * 60)
        report.append("ğŸ† å“ç‰Œæ–°å“ç”»åƒåˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"ğŸ“… åˆ†ææ—¶é—´: {self.analysis_results['analysis_time']}")
        report.append(f"ğŸ“Š åˆ†æå“ç‰Œæ•°: {self.analysis_results['total_brands']}")
        report.append("ğŸ¯ åˆ†æé‡ç‚¹: åŸºäºæ‰€æœ‰æ–°å“å•†å“æ•°æ®")
        report.append("")
        
        for brand, data in self.analysis_results['brand_analyses'].items():
            summary_stats = data.get('summary_stats', {})
            brand_profile = data.get('brand_profile', {})
            
            report.append(f"ğŸ·ï¸  å“ç‰Œ: {brand}")
            report.append(f"   å•†å“æ•°é‡: {summary_stats.get('total_products', 0)}ä¸ª")
            report.append(f"   åˆ†æå•†å“: {summary_stats.get('analyzed_products', 0)}ä¸ª")
            
            price_range = summary_stats.get('price_range', {})
            if price_range.get('min', 0) > 0:
                report.append(f"   ä»·æ ¼åŒºé—´: Â¥{price_range['min']:.0f} - Â¥{price_range['max']:.0f}")
                report.append(f"   å¹³å‡ä»·æ ¼: Â¥{price_range['avg']:.0f}")
            
            sales_perf = summary_stats.get('sales_performance', {})
            if sales_perf.get('avg_sales', 0) > 0:
                report.append(f"   å¹³å‡é”€é‡: {sales_perf['avg_sales']:.0f}")
            
            # æ˜¾ç¤ºå“ç‰Œç”»åƒæ ¸å¿ƒä¿¡æ¯
            if isinstance(brand_profile, dict) and 'market_position' in brand_profile:
                report.append(f"   å¸‚åœºå®šä½: {brand_profile.get('market_position', 'æœªçŸ¥')}")
                report.append(f"   ä»·æ ¼å®šä½: {brand_profile.get('price_positioning', 'æœªçŸ¥')}")
                if brand_profile.get('core_features'):
                    features = brand_profile['core_features'][:2]  # åªæ˜¾ç¤ºå‰2ä¸ªç‰¹è‰²
                    report.append(f"   æ ¸å¿ƒç‰¹è‰²: {', '.join(features)}")
            elif 'error' in brand_profile:
                report.append(f"   çŠ¶æ€: âŒ åˆ†æå¤±è´¥ - {brand_profile.get('error', 'æœªçŸ¥é”™è¯¯')}")
            elif 'raw_analysis' in brand_profile:
                report.append(f"   çŠ¶æ€: åˆ†æå®Œæˆï¼Œæ ¼å¼å¾…ä¼˜åŒ–")
            else:
                report.append(f"   çŠ¶æ€: åˆ†æä¸­...")
            
            report.append("")
        
        return "\n".join(report)
    
    def generate_brand_profile_stats(self) -> str:
        """
        ç”Ÿæˆå“ç‰Œç”»åƒç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡æŠ¥å‘Šæ–‡æœ¬
        """
        if not self.analysis_results:
            return "æ²¡æœ‰åˆ†æç»“æœå¯ç”Ÿæˆç»Ÿè®¡"
        
        stats = []
        stats.append("=" * 60)
        stats.append("ğŸ¨ å“ç‰Œæ–°å“ç”»åƒåˆ†æç»Ÿè®¡æŠ¥å‘Š")
        stats.append("=" * 60)
        
        total_brands = len(self.analysis_results.get('brand_analyses', {}))
        completed_profiles = 0
        failed_profiles = 0
        total_analyzed_products = 0
        
        for brand, data in self.analysis_results.get('brand_analyses', {}).items():
            summary_stats = data.get('summary_stats', {})
            brand_profile = data.get('brand_profile', {})
            
            # ç»Ÿè®¡åˆ†æå•†å“æ•°é‡
            analyzed_products = summary_stats.get('analyzed_products', 0)
            total_analyzed_products += analyzed_products
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„å“ç‰Œç”»åƒ
            if isinstance(brand_profile, dict) and 'market_position' in brand_profile:
                completed_profiles += 1
                stats.append(f"âœ… {brand}: {analyzed_products} ä¸ªæ–°å“")
            elif 'error' in brand_profile:
                failed_profiles += 1
                stats.append(f"âŒ {brand}: åˆ†æå¤±è´¥ - {brand_profile.get('error', 'æœªçŸ¥é”™è¯¯')}")
            else:
                stats.append(f"â³ {brand}: åˆ†æä¸­æˆ–æ ¼å¼å¾…ä¼˜åŒ–")
        
        stats.append("")
        stats.append(f"ğŸ“Š æ€»è®¡:")
        stats.append(f"   - å“ç‰Œæ€»æ•°: {total_brands}")
        stats.append(f"   - å“ç‰Œç”»åƒå®Œæˆ: {completed_profiles}/{total_brands}")
        stats.append(f"   - åˆ†æå¤±è´¥: {failed_profiles}/{total_brands}")
        stats.append(f"   - åˆ†ææ–°å“æ€»æ•°: {total_analyzed_products}")
        stats.append(f"   - å¹³å‡æ¯å“ç‰Œæ–°å“æ•°: {total_analyzed_products/total_brands:.1f}")
        
        coverage_rate = completed_profiles / total_brands * 100 if total_brands > 0 else 0
        stats.append(f"   - å“ç‰Œç”»åƒå®Œæˆç‡: {coverage_rate:.1f}%")
        
        stats.append("")
        stats.append("ğŸ¯ å“ç‰Œæ–°å“ç”»åƒç‰¹ç‚¹:")
        stats.append("   - ğŸ“ åŸºäºæ‰€æœ‰æ–°å“çš„æ ‡é¢˜ã€ä»·æ ¼ã€é”€é‡ç­‰æ–‡æœ¬ä¿¡æ¯")
        stats.append("   - ğŸ†• å…¨é‡æ–°å“æ•°æ®åˆ†æï¼Œæ ¸å¿ƒå…³é”®è¯å’Œç‰¹è‰²æå–")
        stats.append("   - ğŸ›’ ç›´æ¥çš„è´­ä¹°å»ºè®®å’Œé¿å‘æŒ‡å—")
        stats.append("   - ğŸ“Š åŸºäºå®Œæ•´æ–°å“æ•°æ®çš„æ·±åº¦åˆ†æ")
        stats.append("   - ğŸ” é€‚åˆRAGæ£€ç´¢å’Œè´­ç‰©æ¨è")
        stats.append("   - ğŸš€ çªå‡ºæ–°å“ç­–ç•¥ã€åˆ›æ–°ç‰¹è‰²å’Œå¸‚åœºè¶‹åŠ¿")
        
        return "\n".join(stats)
    
    def generate_brand_directory_index(self, results: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆå“ç‰Œç›®å½•ç´¢å¼•æ–‡ä»¶
        
        Args:
            results: åˆ†æç»“æœ
            
        Returns:
            ç´¢å¼•æ–‡ä»¶å†…å®¹
        """
        index_content = []
        index_content.append("å“ç‰Œæ–°å“ç”»åƒåˆ†æç›®å½•ç´¢å¼•")
        index_content.append("=" * 50)
        index_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        index_content.append(f"æ€»å“ç‰Œæ•°: {len(results.get('brand_analyses', {}))}")
        index_content.append("")
        
        # æŒ‰å“ç‰Œåç§°æ’åº
        sorted_brands = sorted(results.get('brand_analyses', {}).keys())
        
        for brand in sorted_brands:
            data = results['brand_analyses'][brand]
            brand_profile = data.get('brand_profile', {})
            summary_stats = data.get('summary_stats', {})
            
            index_content.append(f"ğŸ·ï¸  {brand}")
            index_content.append(f"   å•†å“æ•°é‡: {summary_stats.get('total_products', 0)} ä¸ªæ–°å“")
            
            if 'error' in brand_profile:
                index_content.append(f"   çŠ¶æ€: âŒ åˆ†æå¤±è´¥")
                index_content.append(f"   é”™è¯¯: {brand_profile.get('error', 'æœªçŸ¥é”™è¯¯')}")
            else:
                index_content.append(f"   çŠ¶æ€: âœ… åˆ†ææˆåŠŸ")
                if isinstance(brand_profile, dict) and 'market_position' in brand_profile:
                    index_content.append(f"   å¸‚åœºå®šä½: {brand_profile.get('market_position', 'æœªçŸ¥')}")
                    index_content.append(f"   ä»·æ ¼å®šä½: {brand_profile.get('price_positioning', 'æœªçŸ¥')}")
            
            # æ–‡ä»¶è·¯å¾„ä¿¡æ¯
            safe_brand_name = brand.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
            index_content.append(f"   æ–‡ä»¶: brand_analysis_without_image/{brand}/{safe_brand_name}_latest.json")
            index_content.append("")
        
        return "\n".join(index_content)


def main(date_dir: str,output_dir: str,force_reanalyze: bool=False):
    """ä¸»å‡½æ•° - åŸºäºæ‰€æœ‰æ–°å“å•†å“æ–‡æœ¬ä¿¡æ¯ç”Ÿæˆå“ç‰Œç”»åƒ"""
    date_dir =pathlib.Path(date_dir)
    output_dir = pathlib.Path(output_dir)

    
    try:
        # åˆå§‹åŒ–åˆ†æå™¨ï¼Œä¼ é€’æ—¥æœŸå‚æ•°å’Œå¼ºåˆ¶é‡æ–°åˆ†æé€‰é¡¹
        analyzer = BrandAnalyzer(date_dir=date_dir, force_reanalyze=force_reanalyze)
        
        # ä½¿ç”¨æ‰¹é‡äº§å“ç›®å½•
        batch_dir = date_dir/f"extracted_products_batch"
        if not os.path.exists(batch_dir):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ‰¹é‡äº§å“ç›®å½• {batch_dir}")
            print("   è¯·ç¡®ä¿ç›®å½•å­˜åœ¨ä¸”åŒ…å«å“ç‰Œäº§å“æ–‡ä»¶")
            return
        
        # ä»æ‰¹é‡ç›®å½•åŠ è½½æ‰€æœ‰å“ç‰Œçš„äº§å“æ•°æ®
        brand_data = analyzer.load_products_from_batch_directory(batch_dir)
        
        if not brand_data:
            print("âŒ é”™è¯¯: æ— æ³•åŠ è½½ä»»ä½•å“ç‰Œæ•°æ®")
            return
        
        # åˆ†ææ‰€æœ‰å“ç‰Œ
        results = analyzer.analyze_all_brands()
        
        # ä¿å­˜ç»“æœåˆ°æŒ‡å®šç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºä¸»è¾“å‡ºç›®å½•
        main_output_dir = output_dir
        os.makedirs(main_output_dir, exist_ok=True)
        
        # ä¿å­˜å®Œæ•´åˆ†æç»“æœ
        output_file = f"{main_output_dir}/all_brands_analysis_{timestamp}.json"
        analyzer.save_analysis_results(output_file)
        
        # ç”Ÿæˆå“ç‰Œåˆ†ææ€»ç»“æŠ¥å‘Š
        summary_file = f"{main_output_dir}/brand_analysis_summary_{timestamp}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("å“ç‰Œæ–°å“ç”»åƒåˆ†ææ€»ç»“æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"åˆ†ææ—¶é—´: {timestamp}\n")
            f.write(f"æ€»å“ç‰Œæ•°: {len(results.get('brand_analyses', {}))}\n\n")
            
            # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„å“ç‰Œ
            success_count = 0
            failed_count = 0
            for brand, data in results.get('brand_analyses', {}).items():
                brand_profile = data.get('brand_profile', {})
                if 'error' in brand_profile:
                    failed_count += 1
                    f.write(f"âŒ {brand}: åˆ†æå¤±è´¥\n")
                else:
                    success_count += 1
                    f.write(f"âœ… {brand}: åˆ†ææˆåŠŸ\n")
            
            f.write(f"\næ€»ç»“: æˆåŠŸ {success_count} ä¸ªå“ç‰Œï¼Œå¤±è´¥ {failed_count} ä¸ªå“ç‰Œ\n")
            f.write(f"æˆåŠŸç‡: {success_count/(success_count+failed_count)*100:.1f}%\n")
        
        print(f"ğŸ“‹ åˆ†ææ€»ç»“å·²ä¿å­˜åˆ°: {summary_file}")
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary = analyzer.generate_summary_report()
        print("\n" + summary)
        
        # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
        summary_file = f"{main_output_dir}/brand_analysis_summary_{timestamp}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary)
        print(f"ğŸ“‹ æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")
        
        # ç”Ÿæˆå“ç‰Œç”»åƒç»Ÿè®¡
        profile_stats = analyzer.generate_brand_profile_stats()
        print("\n" + profile_stats)
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        stats_file = f"{main_output_dir}/brand_profile_stats_{timestamp}.txt"
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write(profile_stats)
        print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {stats_file}")
        
        # ç”Ÿæˆå“ç‰Œç›®å½•ç´¢å¼•
        index_content = analyzer.generate_brand_directory_index(results)
        index_file = f"{main_output_dir}/brand_analysis_index_{timestamp}.txt"
        with open(index_file, 'w', encoding='utf-8') as f:
            f.write(index_content)
        print(f"ğŸ“ å“ç‰Œç›®å½•ç´¢å¼•å·²ä¿å­˜åˆ°: {index_file}")
        
        print("\nğŸ‰ å“ç‰Œæ–°å“ç”»åƒåˆ†æå®Œæˆï¼")
        print("ğŸ“ åŸºäºæ‰€æœ‰æ–°å“çš„æ ‡é¢˜ã€ä»·æ ¼ã€é”€é‡ç­‰æ–‡æœ¬ä¿¡æ¯")
        print("ğŸ¨ å…¨é‡æ–°å“æ•°æ®åˆ†æï¼Œè¾“å‡ºæ ¼å¼ç®€æ´å®ç”¨ï¼Œé€‚åˆRAGæ£€ç´¢")
        print("ğŸ›ï¸ ä¸“æ³¨æ–°å“ç­–ç•¥ã€åˆ›æ–°ç‰¹è‰²å’Œå¸‚åœºè¶‹åŠ¿åˆ†æ")
        print(f"ğŸ“ æ¯ä¸ªå“ç‰Œç»“æœå·²å•ç‹¬ä¿å­˜åˆ°: {main_output_dir}/")
        print(f"ğŸ“‹ å®Œæ•´åˆ†æç»“æœ: {output_file}")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main(
        date_dir=fr"data/{place_id}/{monthly_type}/",
        output_dir=fr"data/{place_id}/{monthly_type}//brand_analysis_without_image",
    )
